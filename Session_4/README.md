Log:

Epoch: 1
/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
loss=0.03138097748160362 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.28it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0510, Accuracy: 9839/10000 (98%)


Epoch: 2
loss=0.10007461160421371 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.23it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0400, Accuracy: 9872/10000 (99%)


Epoch: 3
loss=0.03493379428982735 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 30.72it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0303, Accuracy: 9906/10000 (99%)


Epoch: 4
loss=0.012551014311611652 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.75it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0260, Accuracy: 9920/10000 (99%)


Epoch: 5
loss=0.1193610355257988 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.22it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0271, Accuracy: 9913/10000 (99%)


Epoch: 6
loss=0.011536653153598309 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.74it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0241, Accuracy: 9923/10000 (99%)


Epoch: 7
loss=0.034533239901065826 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.61it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0226, Accuracy: 9930/10000 (99%)


Epoch: 8
loss=0.0135843800380826 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.47it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0221, Accuracy: 9933/10000 (99%)


Epoch: 9
loss=0.009775698184967041 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.54it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0241, Accuracy: 9920/10000 (99%)


Epoch: 10
loss=0.10580683499574661 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.56it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0248, Accuracy: 9916/10000 (99%)


Epoch: 11
loss=0.05657403543591499 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.50it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0202, Accuracy: 9928/10000 (99%)


Epoch: 12
loss=0.04281839728355408 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.38it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0196, Accuracy: 9928/10000 (99%)


Epoch: 13
loss=0.04144364595413208 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.50it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0188, Accuracy: 9937/10000 (99%)


Epoch: 14
loss=0.005642508622258902 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.46it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0180, Accuracy: 9940/10000 (99%)


Epoch: 15
loss=0.002786656143143773 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.35it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0171, Accuracy: 9942/10000 (99%)


Epoch: 16
loss=0.006131077650934458 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 30.37it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0185, Accuracy: 9936/10000 (99%)


Epoch: 17
loss=0.02424968220293522 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.65it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0190, Accuracy: 9941/10000 (99%)


Epoch: 18
loss=0.044876500964164734 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.04it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0184, Accuracy: 9946/10000 (99%)


Epoch: 19
loss=0.004705061670392752 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 30.61it/s]
  0%|          | 0/469 [00:00<?, ?it/s]
Test set: Average loss: 0.0183, Accuracy: 9940/10000 (99%)


Epoch: 20
loss=0.03760702535510063 batch_id=468: 100%|██████████| 469/469 [00:16<00:00, 28.48it/s]
Test set: Average loss: 0.0183, Accuracy: 9943/10000 (99%)
